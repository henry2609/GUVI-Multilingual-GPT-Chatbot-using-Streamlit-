{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AD51hv4Ex7oT"
      },
      "outputs": [],
      "source": [
        "# Single Colab cell: Upgraded GUVI multilingual chatbot (Streamlit app) + optional ngrok\n",
        "# --------------------------------------------------------------------\n",
        "# 1) Paste into Google Colab and run.\n",
        "# 2) Replace NGROK_AUTH_TOKEN with your ngrok authtoken (or leave empty to skip tunnel).\n",
        "# 3) Upload your Excel KB to /content/guvi_qa_table (1).xlsx (or change EXCEL_PATH).\n",
        "# --------------------------------------------------------------------\n",
        "\n",
        "# Install dependencies\n",
        "!pip install -q streamlit transformers sentence-transformers langdetect pyngrok pandas openpyxl python-docx datasets accelerate\n",
        "\n",
        "# ---------- Config: set your ngrok token & HF token here (or leave blank) ----------\n",
        "NGROK_AUTH_TOKEN = \"PASTE_YOUR_NGROK_TOKEN_HERE\"\n",
        "HF_TOKEN = \"\"  # If you need to access private HF models or rate-limit, put your token here.\n",
        "\n",
        "# ---------- App file content (app.py) ----------\n",
        "app_code = r'''\n",
        "import os\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "from langdetect import detect\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "# ---------- SETTINGS ----------\n",
        "EXCEL_PATH = \"/content/guvi_qa_table.xlsx\"  # change if different\n",
        "NLLB_MODEL = \"facebook/nllb-200-distilled-600M\"\n",
        "EMBED_MODEL = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "SCORE_THRESHOLD = 0.28\n",
        "\n",
        "# Text generator model (use small default; swap to your fine-tuned HF repo)\n",
        "GEN_MODEL = \"google/flan-t5-small\"   # lightweight default; replace with your fine-tuned model id when ready\n",
        "USE_GENERATOR = True                 # set to False to skip generation and rely solely on KB answer\n",
        "\n",
        "# Optional: set push-to-hf or private model token via HF_TOKEN env var externally\n",
        "HF_TOKEN = os.environ.get(\"HF_TOKEN\", \"\")\n",
        "\n",
        "SHORT_TO_NLLB = {\n",
        "    \"ta\": \"tam_Taml\",\n",
        "    \"hi\": \"hin_Deva\",\n",
        "    \"te\": \"tel_Telu\",\n",
        "    \"ml\": \"mal_Mlym\",\n",
        "    \"en\": \"eng_Latn\"\n",
        "}\n",
        "\n",
        "# ---------- Load KB ----------\n",
        "@st.cache_data\n",
        "def load_kb(path=EXCEL_PATH):\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Excel KB not found at {path}. Upload the file or change EXCEL_PATH.\")\n",
        "    df = pd.read_excel(path)\n",
        "    # Normalize columns if needed\n",
        "    if \"Question\" not in df.columns or \"Answer\" not in df.columns:\n",
        "        cols = [c.lower() for c in df.columns]\n",
        "        qcol = None; acol = None\n",
        "        for c in df.columns:\n",
        "            if c.lower() in (\"question\",\"q\",\"query\",\"prompt\"):\n",
        "                qcol = c\n",
        "            if c.lower() in (\"answer\",\"a\",\"response\",\"reply\"):\n",
        "                acol = c\n",
        "        if qcol and acol:\n",
        "            df = df[[qcol, acol]].rename(columns={qcol:\"Question\", acol:\"Answer\"})\n",
        "        else:\n",
        "            # fallback: duplicate first column\n",
        "            df = df.rename(columns={df.columns[0]:\"Question\"})\n",
        "            df[\"Answer\"] = df[\"Question\"]\n",
        "    df = df.dropna(subset=[\"Question\",\"Answer\"])\n",
        "    df = df.astype({\"Question\": str, \"Answer\": str})\n",
        "    return df.reset_index(drop=True)\n",
        "\n",
        "# ---------- Load models ----------\n",
        "@st.cache_resource\n",
        "def load_translation_model_and_tokenizer():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(NLLB_MODEL)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(NLLB_MODEL)\n",
        "    return tokenizer, model\n",
        "\n",
        "@st.cache_resource\n",
        "def load_embedder():\n",
        "    return SentenceTransformer(EMBED_MODEL)\n",
        "\n",
        "@st.cache_resource\n",
        "def load_generator(model_name=GEN_MODEL, hf_token=None):\n",
        "    # uses transformers pipeline for text2text-generation\n",
        "    pipe = pipeline(\"text2text-generation\", model=model_name, device=0 if torch.cuda.is_available() else -1, use_auth_token=hf_token if hf_token else None)\n",
        "    return pipe\n",
        "\n",
        "# ---------- NLLB translate helper ----------\n",
        "def lang_to_nllb(code_or_short):\n",
        "    if code_or_short in SHORT_TO_NLLB:\n",
        "        return SHORT_TO_NLLB[code_or_short]\n",
        "    return code_or_short\n",
        "\n",
        "def translate_nllb(text, tgt_nllb, src_nllb=None, tokenizer=None, model=None):\n",
        "    if tokenizer is None or model is None:\n",
        "        tokenizer, model = load_translation_model_and_tokenizer()\n",
        "    bos_id = None\n",
        "    try:\n",
        "        bos_id = tokenizer.lang_code_to_id.get(tgt_nllb, None)\n",
        "    except Exception:\n",
        "        bos_id = None\n",
        "    if bos_id is None:\n",
        "        token_string_candidates = [f\"<{tgt_nllb}>\", tgt_nllb]\n",
        "        for tokstr in token_string_candidates:\n",
        "            tok_id = tokenizer.convert_tokens_to_ids(tokstr)\n",
        "            if tok_id != tokenizer.unk_token_id:\n",
        "                bos_id = tok_id\n",
        "                break\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    gen_kwargs = {\"max_length\": 512}\n",
        "    if bos_id is not None and bos_id != tokenizer.unk_token_id:\n",
        "        gen_kwargs[\"forced_bos_token_id\"] = bos_id\n",
        "    translated = model.generate(**inputs, **gen_kwargs)\n",
        "    out = tokenizer.batch_decode(translated, skip_special_tokens=True)[0]\n",
        "    return out\n",
        "\n",
        "# ---------- Embedding prep & search ----------\n",
        "@st.cache_resource\n",
        "def prepare_embeddings(kb_questions):\n",
        "    embedder = load_embedder()\n",
        "    return embedder.encode(kb_questions, convert_to_tensor=True)\n",
        "\n",
        "def semantic_search(query_en, kb_questions, kb_embs, top_k=3):\n",
        "    embedder = load_embedder()\n",
        "    q_emb = embedder.encode(query_en, convert_to_tensor=True)\n",
        "    scores = util.pytorch_cos_sim(q_emb, kb_embs)[0].cpu().numpy()\n",
        "    idxs = np.argsort(-scores)[:top_k]\n",
        "    return [(int(i), float(scores[i]), kb_questions[int(i)]) for i in idxs]\n",
        "\n",
        "# ---------- Utility: build generator prompt ----------\n",
        "def build_generation_prompt(user_query, top_contexts):\n",
        "    # Create a concise prompt containing top KB answers as context\n",
        "    ctx_text = \"\\n\\n\".join([f\"Context {i+1} Q: {q}\\nA: {a}\" for i,(q,a) in enumerate(top_contexts)])\n",
        "    prompt = f\"You are a helpful assistant for GUVI. Use the following context to answer the user's question.\\n\\n{ctx_text}\\n\\nUser question: {user_query}\\n\\nAnswer concisely and mention if the KB doesn't contain enough info.\"\n",
        "    return prompt\n",
        "\n",
        "# ---------- Streamlit UI ----------\n",
        "st.set_page_config(page_title=\"GUVI Multilingual Chatbot (Upgraded)\", layout=\"wide\")\n",
        "st.title(\"ðŸ¤– GUVI Multilingual Chatbot â€” Upgraded (KB + Generator + Logging)\")\n",
        "\n",
        "# Load KB and models\n",
        "try:\n",
        "    df = load_kb()\n",
        "except FileNotFoundError as e:\n",
        "    st.error(str(e))\n",
        "    st.stop()\n",
        "\n",
        "tokenizer_nllb, nllb_model = load_translation_model_and_tokenizer()\n",
        "embedder = load_embedder()\n",
        "questions = df[\"Question\"].astype(str).tolist()\n",
        "answers = df[\"Answer\"].astype(str).tolist()\n",
        "kb_embs = prepare_embeddings(questions)\n",
        "\n",
        "# Generator (optional)\n",
        "gen_pipe = None\n",
        "if USE_GENERATOR:\n",
        "    try:\n",
        "        gen_pipe = load_generator(GEN_MODEL, hf_token=os.environ.get(\"HF_TOKEN\",\"\") or None)\n",
        "    except Exception as ex:\n",
        "        st.warning(f\"Failed to load generator model {GEN_MODEL}: {ex}\")\n",
        "        gen_pipe = None\n",
        "\n",
        "st.sidebar.header(\"KB & Model Info\")\n",
        "st.sidebar.write(f\"Loaded {len(questions)} Q/A rows.\")\n",
        "st.sidebar.write(f\"Embedding model: {EMBED_MODEL}\")\n",
        "st.sidebar.write(f\"Translation model: {NLLB_MODEL}\")\n",
        "st.sidebar.write(f\"Generator model: {GEN_MODEL} (enabled={USE_GENERATOR})\")\n",
        "if st.sidebar.checkbox(\"Show sample rows\"):\n",
        "    st.sidebar.write(df.head())\n",
        "\n",
        "user_input = st.text_input(\"Ask about GUVI (type in Tamil/Hindi/Telugu/Malayalam/English):\", \"\")\n",
        "\n",
        "if st.button(\"Ask\") and user_input.strip():\n",
        "    # detect language\n",
        "    try:\n",
        "        detected_short = detect(user_input)\n",
        "    except Exception:\n",
        "        detected_short = \"en\"\n",
        "    src_nllb = lang_to_nllb(detected_short)\n",
        "    tgt_nllb = \"eng_Latn\"\n",
        "\n",
        "    # Translate to English for search\n",
        "    if detected_short != \"en\":\n",
        "        try:\n",
        "            query_en = translate_nllb(user_input, tgt_nllb, src_nllb, tokenizer=tokenizer_nllb, model=nllb_model)\n",
        "        except Exception:\n",
        "            # fallback simple pipeline if available\n",
        "            try:\n",
        "                small_pipe = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-mul-en\")\n",
        "                query_en = small_pipe(user_input)[0][\"translation_text\"]\n",
        "            except Exception:\n",
        "                query_en = user_input\n",
        "    else:\n",
        "        query_en = user_input\n",
        "\n",
        "    # Semantic search top_k\n",
        "    top_k = st.sidebar.slider(\"Top-K contexts for generator\", 1, 6, 3)\n",
        "    matches = semantic_search(query_en, questions, kb_embs, top_k=top_k)\n",
        "    top_contexts = [(questions[i], answers[i]) for i,_,_ in matches]\n",
        "\n",
        "    best_idx, best_score, best_q = matches[0]\n",
        "    best_answer_en = answers[best_idx]\n",
        "\n",
        "    # Decide whether to use KB answer or generator\n",
        "    final_answer_en = best_answer_en\n",
        "    used_generator = False\n",
        "    if USE_GENERATOR and gen_pipe is not None:\n",
        "        # if confidence low, or user wants generator, call generator to produce an answer using context\n",
        "        gen_trigger = st.sidebar.selectbox(\"Answer source\", [\"Auto (KB unless low confidence)\",\"Always use generator\",\"KB only\"])\n",
        "        if gen_trigger == \"Always use generator\" or best_score < SCORE_THRESHOLD:\n",
        "            prompt = build_generation_prompt(query_en, top_contexts)\n",
        "            try:\n",
        "                gen_out = gen_pipe(prompt, max_length=256, do_sample=False)\n",
        "                final_answer_en = gen_out[0][\"generated_text\"] if isinstance(gen_out, list) else str(gen_out)\n",
        "                used_generator = True\n",
        "            except Exception as ex:\n",
        "                st.warning(f\"Generator failed: {ex}. Falling back to KB answer.\")\n",
        "                final_answer_en = best_answer_en\n",
        "                used_generator = False\n",
        "        else:\n",
        "            # KB answer used\n",
        "            final_answer_en = best_answer_en\n",
        "    else:\n",
        "        # generator disabled or not loaded; rely on KB\n",
        "        final_answer_en = best_answer_en\n",
        "\n",
        "    # Translate back to user's language if needed\n",
        "    if detected_short != \"en\":\n",
        "        try:\n",
        "            answer_local = translate_nllb(final_answer_en, lang_to_nllb(detected_short), src_nllb=\"eng_Latn\", tokenizer=tokenizer_nllb, model=nllb_model)\n",
        "        except Exception:\n",
        "            try:\n",
        "                fallback_pipe = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-mul\")\n",
        "                answer_local = fallback_pipe(final_answer_en)[0][\"translation_text\"]\n",
        "            except Exception:\n",
        "                answer_local = final_answer_en\n",
        "    else:\n",
        "        answer_local = final_answer_en\n",
        "\n",
        "    # Display confidence and answer\n",
        "    if best_score < SCORE_THRESHOLD:\n",
        "        st.info(f\"Low KB confidence (score={best_score:.3f}).\")\n",
        "    st.markdown(\"**Answer:**\")\n",
        "    st.write(answer_local)\n",
        "\n",
        "    # Show debug matches if wanted\n",
        "    if st.checkbox(\"Show top matches (debug)\"):\n",
        "        for i, s, txt in matches:\n",
        "            st.write(f\"- (score={s:.3f}) Q: {questions[i]}\")\n",
        "\n",
        "    # Logging for evaluation\n",
        "    log_row = {\n",
        "        \"timestamp\": datetime.utcnow().isoformat(),\n",
        "        \"user_query\": user_input,\n",
        "        \"query_en\": query_en,\n",
        "        \"detected_lang\": detected_short,\n",
        "        \"best_score\": best_score,\n",
        "        \"best_q_idx\": int(best_idx),\n",
        "        \"used_generator\": used_generator,\n",
        "        \"answer_en\": final_answer_en,\n",
        "        \"answer_local\": answer_local\n",
        "    }\n",
        "    log_path = \"/content/guvi_chat_logs.csv\"\n",
        "    df_logs = pd.DataFrame([log_row])\n",
        "    if os.path.exists(log_path):\n",
        "        df_logs.to_csv(log_path, mode=\"a\", header=False, index=False)\n",
        "    else:\n",
        "        df_logs.to_csv(log_path, index=False)\n",
        "    st.sidebar.write(f\"Logged query to {log_path}\")\n",
        "'''\n",
        "\n",
        "# Write app.py to disk\n",
        "with open(\"app.py\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(app_code)\n",
        "\n",
        "# ---------- Create requirements.txt ----------\n",
        "requirements = \"\"\"streamlit\n",
        "transformers\n",
        "sentence-transformers\n",
        "langdetect\n",
        "pyngrok\n",
        "pandas\n",
        "openpyxl\n",
        "python-docx\n",
        "datasets\n",
        "accelerate\n",
        "\"\"\"\n",
        "with open(\"requirements.txt\", \"w\") as f:\n",
        "    f.write(requirements)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import time\n",
        "import subprocess\n",
        "\n",
        "# Kill any old tunnels and processes\n",
        "ngrok.kill()\n",
        "\n",
        "# Start Streamlit app in background\n",
        "process = subprocess.Popen([\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\"])\n",
        "\n",
        "# Wait a bit for Streamlit to start\n",
        "time.sleep(5)\n",
        "\n",
        "# Start ngrok tunnel\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"âœ… Public Streamlit URL:\", public_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXZRnslczGp7",
        "outputId": "23855cb7-d24a-4f0f-86a6-865fac1de978"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Public Streamlit URL: NgrokTunnel: \"https://be43c878d5bd.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ]
}